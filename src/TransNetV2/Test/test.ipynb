{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4d6daf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/81/f9z4jcsj7gz1v7bs26nb55jm0000gn/T/ipykernel_85540/1087648453.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"transnetv2-pytorch-weights.pth\")\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python\n",
    "%pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c6b1128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import ffmpeg\n",
    "from transnet import TransNetV2\n",
    "from inference import predict_video\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "\n",
    "model = TransNetV2()\n",
    "state_dict = torch.load(\"transnetv2-pytorch-weights.pth\")\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d50ff4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying for ../../Test_dataset/00112.mp4\n",
      "[TransNetV2] Processing video frames 7676/7676Detected scenes: [[   0   71]\n",
      " [  73  134]\n",
      " [ 135  217]\n",
      " [ 218  302]\n",
      " [ 303  366]\n",
      " [ 367  460]\n",
      " [ 461  500]\n",
      " [ 501  533]\n",
      " [ 534  567]\n",
      " [ 568  616]\n",
      " [ 617  665]\n",
      " [ 666  698]\n",
      " [ 699  765]\n",
      " [ 766  795]\n",
      " [ 796  831]\n",
      " [ 832  865]\n",
      " [ 866  897]\n",
      " [ 898  934]\n",
      " [ 935  983]\n",
      " [ 984 1028]\n",
      " [1029 1126]\n",
      " [1127 1157]\n",
      " [1158 1195]\n",
      " [1196 1232]\n",
      " [1233 1263]\n",
      " [1264 1288]\n",
      " [1289 1468]\n",
      " [1469 1492]\n",
      " [1493 1528]\n",
      " [1529 1560]\n",
      " [1561 1630]\n",
      " [1631 1658]\n",
      " [1659 1694]\n",
      " [1695 1728]\n",
      " [1729 1820]\n",
      " [1821 1838]\n",
      " [1839 1853]\n",
      " [1854 1884]\n",
      " [1885 1926]\n",
      " [1927 1947]\n",
      " [1948 2004]\n",
      " [2005 2009]\n",
      " [2010 2019]\n",
      " [2020 2050]\n",
      " [2051 2091]\n",
      " [2092 2113]\n",
      " [2114 2154]\n",
      " [2155 2169]\n",
      " [2170 2195]\n",
      " [2196 2236]\n",
      " [2237 2277]\n",
      " [2278 2298]\n",
      " [2299 2414]\n",
      " [2415 2464]\n",
      " [2465 2500]\n",
      " [2501 2517]\n",
      " [2518 2546]\n",
      " [2547 2599]\n",
      " [2600 2641]\n",
      " [2642 2724]\n",
      " [2725 2763]\n",
      " [2764 2795]\n",
      " [2796 2831]\n",
      " [2832 2857]\n",
      " [2858 2877]\n",
      " [2878 2899]\n",
      " [2900 2938]\n",
      " [2939 2959]\n",
      " [2960 2982]\n",
      " [2983 2997]\n",
      " [2998 3019]\n",
      " [3020 3043]\n",
      " [3044 3064]\n",
      " [3065 3085]\n",
      " [3086 3106]\n",
      " [3107 3147]\n",
      " [3148 3162]\n",
      " [3163 3187]\n",
      " [3188 3209]\n",
      " [3210 3229]\n",
      " [3230 3313]\n",
      " [3314 3354]\n",
      " [3355 3395]\n",
      " [3396 3417]\n",
      " [3418 3438]\n",
      " [3439 3458]\n",
      " [3459 3510]\n",
      " [3511 3559]\n",
      " [3560 3603]\n",
      " [3604 3686]\n",
      " [3687 3707]\n",
      " [3708 3729]\n",
      " [3730 3750]\n",
      " [3751 3769]\n",
      " [3770 3790]\n",
      " [3791 3818]\n",
      " [3819 3888]\n",
      " [3889 3945]\n",
      " [3946 3988]\n",
      " [3989 4026]\n",
      " [4027 4076]\n",
      " [4077 4128]\n",
      " [4129 4235]\n",
      " [4236 4263]\n",
      " [4264 4291]\n",
      " [4292 4337]\n",
      " [4338 4402]\n",
      " [4403 4440]\n",
      " [4441 4484]\n",
      " [4485 4513]\n",
      " [4514 4546]\n",
      " [4547 4585]\n",
      " [4586 4601]\n",
      " [4602 4661]\n",
      " [4662 4744]\n",
      " [4745 4771]\n",
      " [4772 4806]\n",
      " [4807 4843]\n",
      " [4844 4913]\n",
      " [4914 4982]\n",
      " [4983 5030]\n",
      " [5031 5069]\n",
      " [5070 5125]\n",
      " [5126 5167]\n",
      " [5168 5194]\n",
      " [5195 5240]\n",
      " [5241 5280]\n",
      " [5281 5310]\n",
      " [5311 5357]\n",
      " [5358 5396]\n",
      " [5397 5436]\n",
      " [5437 5483]\n",
      " [5484 5508]\n",
      " [5509 5543]\n",
      " [5544 5559]\n",
      " [5560 5588]\n",
      " [5589 5616]\n",
      " [5617 5675]\n",
      " [5676 5727]\n",
      " [5728 5776]\n",
      " [5777 5810]\n",
      " [5811 5882]\n",
      " [5883 5910]\n",
      " [5911 6034]\n",
      " [6035 6063]\n",
      " [6064 6088]\n",
      " [6089 6218]\n",
      " [6219 6242]\n",
      " [6243 6285]\n",
      " [6286 6302]\n",
      " [6303 6327]\n",
      " [6328 6351]\n",
      " [6352 6401]\n",
      " [6402 6458]\n",
      " [6459 6493]\n",
      " [6494 6616]\n",
      " [6617 6639]\n",
      " [6640 6687]\n",
      " [6688 6737]\n",
      " [6738 6815]\n",
      " [6816 6883]\n",
      " [6884 6916]\n",
      " [6917 7004]\n",
      " [7005 7055]\n",
      " [7056 7099]\n",
      " [7100 7206]\n",
      " [7207 7257]\n",
      " [7258 7420]\n",
      " [7422 7663]\n",
      " [7664 7675]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # shape: batch dim x video frames x frame height x frame width x RGB (not BGR) channels\n",
    "    scenes = predict_video('../../Test_dataset/00112.mp4', model)\n",
    "    print(\"Detected scenes:\", scenes)\n",
    "    #visualize_detection('../../Dataset/00100.mp4', scenes,'./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9f82cf",
   "metadata": {},
   "outputs": [
    {
     
    }
   ],
   "source": [
    "\n",
    "\n",
    "def extract_frame(video_path, frame_idx):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        return None\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(frame_rgb)\n",
    "\n",
    "def visualize_shots_scrollable(video_path, scenes_data):\n",
    "    if isinstance(scenes_data, torch.Tensor):\n",
    "        scenes_data = scenes_data.cpu().numpy()\n",
    "    elif isinstance(scenes_data, list):\n",
    "        scenes_data = np.array(scenes_data)\n",
    "\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Shot Boundary Viewer\")\n",
    "    def on_closing():\n",
    "        root.quit()\n",
    "        root.destroy()\n",
    "\n",
    "    root.protocol(\"WM_DELETE_WINDOW\", on_closing)\n",
    "\n",
    "    main_frame = ttk.Frame(root)\n",
    "    main_frame.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "    canvas = tk.Canvas(main_frame)\n",
    "    scrollbar = ttk.Scrollbar(main_frame, orient=tk.VERTICAL, command=canvas.yview)\n",
    "    scrollable_frame = ttk.Frame(canvas)\n",
    "\n",
    "    scrollable_frame.bind(\n",
    "        \"<Configure>\",\n",
    "        lambda e: canvas.configure(\n",
    "            scrollregion=canvas.bbox(\"all\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    canvas.create_window((0, 0), window=scrollable_frame, anchor=\"nw\")\n",
    "    canvas.configure(yscrollcommand=scrollbar.set)\n",
    "\n",
    "    canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "    scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "\n",
    "    for idx, (start, end) in enumerate(scenes_data):\n",
    "        start_img = extract_frame(video_path, int(start))\n",
    "        end_img = extract_frame(video_path, int(end))\n",
    "\n",
    "        if start_img is None or end_img is None:\n",
    "            continue\n",
    "\n",
    "        start_img_tk = ImageTk.PhotoImage(start_img.resize((160, 90)))\n",
    "        end_img_tk = ImageTk.PhotoImage(end_img.resize((160, 90)))\n",
    "\n",
    "        pair_frame = ttk.Frame(scrollable_frame, padding=10)\n",
    "        pair_frame.pack(fill=tk.X)\n",
    "\n",
    "        ttk.Label(pair_frame, text=f\"Shot {idx+1}: [{start}-{end}]\").pack(anchor=\"w\")\n",
    "\n",
    "        images_frame = ttk.Frame(pair_frame)\n",
    "        images_frame.pack()\n",
    "\n",
    "        ttk.Label(images_frame, image=start_img_tk).pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Label(images_frame, image=end_img_tk).pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        # Store reference to avoid garbage collection\n",
    "        pair_frame.start_img_tk = start_img_tk\n",
    "        pair_frame.end_img_tk = end_img_tk\n",
    "\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    video_file = '../../Dataset/00001.mp4'\n",
    "    #example_scenes = [[0, 433], [434, 495], [496, 553], [554, 646]]\n",
    "    with torch.no_grad():\n",
    "    # shape: batch dim x video frames x frame height x frame width x RGB (not BGR) channels\n",
    "        scenes = predict_video(video_file, model)\n",
    "    scenes_np = np.array(scenes)\n",
    "    print(os.path.exists(\"00112.mp4\"))\n",
    "    visualize_shots_scrollable(\"00112.mp4\", scenes_np)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6facc278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([1, 768])\n",
      "Text features: torch.Size([1, 768])\n",
      "tensor(-0.4855) tensor(0.5251) tensor(-0.0007)\n",
      "Diff:  0.20456713438034058\n",
      "Image features: [[1.0000000e+00 1.0508931e-08]]\n",
      "Top predictions: [[1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"cat.jpg\")).unsqueeze(0).to(device)\n",
    "image1 = preprocess(Image.open(\"car.png\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a photo of a white and brown cat\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "    print(\"Image features:\", image_features.shape)\n",
    "    print(\"Text features:\", text_features.shape)\n",
    "    im = torch.nn.functional.normalize(image_features[0], dim=0)\n",
    "    print(im.min(), im.max(), im.mean())\n",
    "    print(\"Diff: \", torch.nn.functional.cosine_similarity(im , torch.nn.functional.normalize(text_features, dim=0), dim=-1).item())\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(torch.cat([image, image1], dim=0), text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "    probs1 = logits_per_text.softmax(dim=-1).cpu().numpy()\n",
    "    print(\"Image features:\", probs1)\n",
    "\n",
    "print(\"Top predictions:\", probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f19f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([1, 768])\n",
      "Text features: torch.Size([1, 768])\n",
      "tensor(-0.4855) tensor(0.5251) tensor(-0.0007)\n",
      "Diff:  0.26497387886047363\n",
      "Image features: [[1.]]\n",
      "Top predictions: [[1.]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"cat.jpg\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a white and brown cat\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "    print(\"Image features:\", image_features.shape)\n",
    "    print(\"Text features:\", text_features.shape)\n",
    "    im_norm = torch.nn.functional.normalize(image_features, p=2, dim=1)\n",
    "    txt_norm = torch.nn.functional.normalize(text_features, p=2,dim=1)\n",
    "    print(\"Diff: \", torch.nn.functional.cosine_similarity(im_norm , txt_norm).item())\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "    probs1 = logits_per_text.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Image features:\", probs1)\n",
    "print(\"Top predictions:\", probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb7fe392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance by FAISS:[[0.26497385]]\n",
      "Cosine similarity: 0.26497385695498765\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import faiss.contrib.torch_utils\n",
    "\n",
    "# x_torch = torch.nn.functional.normalize(image_features, p=2, dim=1)\n",
    "# q_torch = torch.nn.functional.normalize(text_features, p=2, dim=1)\n",
    "\n",
    "x_torch = im_norm.clone()\n",
    "q_torch = txt_norm.clone()\n",
    "\n",
    "\n",
    "# If you need them back as numpy arrays for faiss\n",
    "x = x_torch.numpy(force=True)\n",
    "q = q_torch.numpy(force=True)\n",
    "\n",
    "index = faiss.IndexFlatIP(x.shape[1])\n",
    "index.add(x)\n",
    "distance, idx = index.search(q, 1)\n",
    "print('Distance by FAISS:{}'.format(distance))\n",
    "\n",
    "# Cosine similarity for comparison\n",
    "result = 1 - scipy.spatial.distance.cosine(x[0], q[0])\n",
    "print('Cosine similarity:', result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
