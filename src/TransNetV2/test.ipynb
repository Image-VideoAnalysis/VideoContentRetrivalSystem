{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d6daf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import ffmpeg\n",
    "from transnet import TransNetV2\n",
    "from inference import predict_video\n",
    "\n",
    "model = TransNetV2()\n",
    "state_dict = torch.load(\"transnetv2-pytorch-weights.pth\")\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d50ff4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TransNetV2] Processing video frames 6480/6480Detected scenes: [[   0  433]\n",
      " [ 434  495]\n",
      " [ 496  553]\n",
      " [ 554  646]\n",
      " [ 647  809]\n",
      " [ 810  972]\n",
      " [ 973 1041]\n",
      " [1042 1110]\n",
      " [1111 1217]\n",
      " [1218 1497]\n",
      " [1498 1564]\n",
      " [1565 1617]\n",
      " [1618 1682]\n",
      " [1683 1725]\n",
      " [1726 2170]\n",
      " [2171 2212]\n",
      " [2213 2279]\n",
      " [2280 2338]\n",
      " [2339 2424]\n",
      " [2425 2572]\n",
      " [2573 2669]\n",
      " [2670 2754]\n",
      " [2755 2826]\n",
      " [2827 3236]\n",
      " [3237 3341]\n",
      " [3342 4011]\n",
      " [4012 4083]\n",
      " [4084 4144]\n",
      " [4145 4204]\n",
      " [4205 4299]\n",
      " [4300 4387]\n",
      " [4388 4413]\n",
      " [4414 4471]\n",
      " [4472 4579]\n",
      " [4580 4639]\n",
      " [4640 4832]\n",
      " [4833 4896]\n",
      " [4897 5115]\n",
      " [5116 5252]\n",
      " [5253 5444]\n",
      " [5445 5557]\n",
      " [5558 5658]\n",
      " [5659 5677]\n",
      " [5678 5770]\n",
      " [5771 5871]\n",
      " [5872 6012]\n",
      " [6013 6070]\n",
      " [6071 6256]\n",
      " [6257 6448]\n",
      " [6449 6479]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # shape: batch dim x video frames x frame height x frame width x RGB (not BGR) channels\n",
    "    scenes = predict_video('../../Dataset/00100.mp4', model)\n",
    "    print(\"Detected scenes:\", scenes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VCA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
