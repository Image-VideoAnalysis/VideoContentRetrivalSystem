{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d6daf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manuc\\AppData\\Local\\Temp\\ipykernel_3772\\1087648453.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"transnetv2-pytorch-weights.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import ffmpeg\n",
    "from transnet import TransNetV2\n",
    "from inference import predict_video\n",
    "\n",
    "model = TransNetV2()\n",
    "state_dict = torch.load(\"transnetv2-pytorch-weights.pth\")\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d50ff4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TransNetV2] Processing video frames 6480/6480Detected scenes: [[   0  433]\n",
      " [ 434  495]\n",
      " [ 496  553]\n",
      " [ 554  646]\n",
      " [ 647  809]\n",
      " [ 810  972]\n",
      " [ 973 1041]\n",
      " [1042 1110]\n",
      " [1111 1217]\n",
      " [1218 1497]\n",
      " [1498 1564]\n",
      " [1565 1617]\n",
      " [1618 1682]\n",
      " [1683 1725]\n",
      " [1726 2170]\n",
      " [2171 2212]\n",
      " [2213 2279]\n",
      " [2280 2338]\n",
      " [2339 2424]\n",
      " [2425 2572]\n",
      " [2573 2669]\n",
      " [2670 2754]\n",
      " [2755 2826]\n",
      " [2827 3236]\n",
      " [3237 3341]\n",
      " [3342 4011]\n",
      " [4012 4083]\n",
      " [4084 4144]\n",
      " [4145 4204]\n",
      " [4205 4299]\n",
      " [4300 4387]\n",
      " [4388 4413]\n",
      " [4414 4471]\n",
      " [4472 4579]\n",
      " [4580 4639]\n",
      " [4640 4832]\n",
      " [4833 4896]\n",
      " [4897 5115]\n",
      " [5116 5252]\n",
      " [5253 5444]\n",
      " [5445 5557]\n",
      " [5558 5658]\n",
      " [5659 5677]\n",
      " [5678 5770]\n",
      " [5771 5871]\n",
      " [5872 6012]\n",
      " [6013 6070]\n",
      " [6071 6256]\n",
      " [6257 6448]\n",
      " [6449 6479]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # shape: batch dim x video frames x frame height x frame width x RGB (not BGR) channels\n",
    "    scenes = predict_video('../../Dataset/00100.mp4', model)\n",
    "    print(\"Detected scenes:\", scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10881188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.11.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc9f82cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TransNetV2] Processing video frames 6824/6824"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "\n",
    "def extract_frame(video_path, frame_idx):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        return None\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(frame_rgb)\n",
    "\n",
    "def visualize_shots_scrollable(video_path, scenes_data):\n",
    "    if isinstance(scenes_data, torch.Tensor):\n",
    "        scenes_data = scenes_data.cpu().numpy()\n",
    "    elif isinstance(scenes_data, list):\n",
    "        scenes_data = np.array(scenes_data)\n",
    "\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Shot Boundary Viewer\")\n",
    "    def on_closing():\n",
    "        root.quit()\n",
    "        root.destroy()\n",
    "\n",
    "    root.protocol(\"WM_DELETE_WINDOW\", on_closing)\n",
    "\n",
    "    main_frame = ttk.Frame(root)\n",
    "    main_frame.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "    canvas = tk.Canvas(main_frame)\n",
    "    scrollbar = ttk.Scrollbar(main_frame, orient=tk.VERTICAL, command=canvas.yview)\n",
    "    scrollable_frame = ttk.Frame(canvas)\n",
    "\n",
    "    scrollable_frame.bind(\n",
    "        \"<Configure>\",\n",
    "        lambda e: canvas.configure(\n",
    "            scrollregion=canvas.bbox(\"all\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    canvas.create_window((0, 0), window=scrollable_frame, anchor=\"nw\")\n",
    "    canvas.configure(yscrollcommand=scrollbar.set)\n",
    "\n",
    "    canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "    scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "\n",
    "    for idx, (start, end) in enumerate(scenes_data):\n",
    "        start_img = extract_frame(video_path, int(start))\n",
    "        end_img = extract_frame(video_path, int(end))\n",
    "\n",
    "        if start_img is None or end_img is None:\n",
    "            continue\n",
    "\n",
    "        start_img_tk = ImageTk.PhotoImage(start_img.resize((160, 90)))\n",
    "        end_img_tk = ImageTk.PhotoImage(end_img.resize((160, 90)))\n",
    "\n",
    "        pair_frame = ttk.Frame(scrollable_frame, padding=10)\n",
    "        pair_frame.pack(fill=tk.X)\n",
    "\n",
    "        ttk.Label(pair_frame, text=f\"Shot {idx+1}: [{start}-{end}]\").pack(anchor=\"w\")\n",
    "\n",
    "        images_frame = ttk.Frame(pair_frame)\n",
    "        images_frame.pack()\n",
    "\n",
    "        ttk.Label(images_frame, image=start_img_tk).pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Label(images_frame, image=end_img_tk).pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        # Store reference to avoid garbage collection\n",
    "        pair_frame.start_img_tk = start_img_tk\n",
    "        pair_frame.end_img_tk = end_img_tk\n",
    "\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    video_file = '../../Dataset/00001.mp4'\n",
    "    #example_scenes = [[0, 433], [434, 495], [496, 553], [554, 646]]\n",
    "    with torch.no_grad():\n",
    "    # shape: batch dim x video frames x frame height x frame width x RGB (not BGR) channels\n",
    "        scenes = predict_video(video_file, model)\n",
    "    scenes_np = np.array(scenes)\n",
    "\n",
    "    if os.path.exists(video_file):\n",
    "        visualize_shots_scrollable(video_file, scenes_np)\n",
    "    else:\n",
    "        print(f\"Video file not found: {video_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6facc278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features: torch.Size([1, 768])\n",
      "Text features: torch.Size([1, 768])\n",
      "tensor(-0.4855) tensor(0.5251) tensor(-0.0007)\n",
      "Diff:  0.17194582521915436\n",
      "Image features: [[9.9999309e-01 6.9734533e-06]]\n",
      "Top predictions: [[1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"cat.jpg\")).unsqueeze(0).to(device)\n",
    "#image1 = preprocess(Image.open(\"car.png\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a cat\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "    print(\"Image features:\", image_features.shape)\n",
    "    print(\"Text features:\", text_features.shape)\n",
    "    im = torch.nn.functional.normalize(image_features[0], dim=0)\n",
    "    print(im.min(), im.max(), im.mean())\n",
    "    print(\"Diff: \", torch.nn.functional.cosine_similarity(im , torch.nn.functional.normalize(text_features, dim=0), dim=-1).item())\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(torch.cat([image, image1], dim=0), text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "    probs1 = logits_per_text.softmax(dim=-1).cpu().numpy()\n",
    "    print(\"Image features:\", probs1)\n",
    "\n",
    "print(\"Top predictions:\", probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eb7fe392",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "input not a numpy array",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m/var/folders/81/f9z4jcsj7gz1v7bs26nb55jm0000gn/T/ipykernel_52466/3155852497.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m x = x_torch.numpy(force=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     11\u001b[39m q = q_torch.numpy(force=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m index = faiss.IndexFlatIP(x.shape[\u001b[32m1\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m index.add(x)\n\u001b[32m     15\u001b[39m distance, idx = index.search(q, \u001b[32m5\u001b[39m)\n\u001b[32m     16\u001b[39m print(\u001b[33m'Distance by FAISS:{}'\u001b[39m.format(distance))\n\u001b[32m     17\u001b[39m \n",
      "\u001b[32m~/miniconda3/lib/python3.12/site-packages/faiss/contrib/torch_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m torch_replacement_add(self, x):\n\u001b[32m    128\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m type(x) \u001b[38;5;28;01mis\u001b[39;00m np.ndarray:\n\u001b[32m    129\u001b[39m             \u001b[38;5;66;03m# forward to faiss __init__.py base method\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.add_numpy(x)\n\u001b[32m    131\u001b[39m \n\u001b[32m    132\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m type(x) \u001b[38;5;28;01mis\u001b[39;00m torch.Tensor\n\u001b[32m    133\u001b[39m         n, d = x.shape\n",
      "\u001b[32m~/miniconda3/lib/python3.12/site-packages/faiss/class_wrappers.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    226\u001b[39m \n\u001b[32m    227\u001b[39m         n, d = x.shape\n\u001b[32m    228\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m d == self.d\n\u001b[32m    229\u001b[39m         x = np.ascontiguousarray(x, dtype=\u001b[33m'float32'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m         self.add_c(n, swig_ptr(x))\n",
      "\u001b[32m~/miniconda3/lib/python3.12/site-packages/faiss/swigfaiss.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(a)\u001b[39m\n\u001b[32m  12252\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m swig_ptr(a):\n\u001b[32m> \u001b[39m\u001b[32m12253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _swigfaiss.swig_ptr(a)\n",
      "\u001b[31mValueError\u001b[39m: input not a numpy array"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import scipy\n",
    "import faiss.contrib.torch_utils\n",
    "\n",
    "x_torch = torch.nn.functional.normalize(image_features, p=2, dim=1)\n",
    "q_torch = torch.nn.functional.normalize(text_features, p=2, dim=1)\n",
    "\n",
    "# If you need them back as numpy arrays for faiss\n",
    "x = x_torch.numpy(force=True)\n",
    "q = q_torch.numpy(force=True)\n",
    "\n",
    "index = faiss.IndexFlatIP(x.shape[1])\n",
    "index.add(x)\n",
    "distance, idx = index.search(q, 5)\n",
    "print('Distance by FAISS:{}'.format(distance))\n",
    "\n",
    "# Cosine similarity for comparison\n",
    "result = 1 - scipy.spatial.distance.cosine(x[0], q[0])\n",
    "print('Cosine similarity:', result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VCA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
