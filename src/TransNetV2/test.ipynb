{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d6daf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manuc\\AppData\\Local\\Temp\\ipykernel_3772\\1087648453.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"transnetv2-pytorch-weights.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import ffmpeg\n",
    "from transnet import TransNetV2\n",
    "from inference import predict_video\n",
    "\n",
    "model = TransNetV2()\n",
    "state_dict = torch.load(\"transnetv2-pytorch-weights.pth\")\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d50ff4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TransNetV2] Processing video frames 6480/6480Detected scenes: [[   0  433]\n",
      " [ 434  495]\n",
      " [ 496  553]\n",
      " [ 554  646]\n",
      " [ 647  809]\n",
      " [ 810  972]\n",
      " [ 973 1041]\n",
      " [1042 1110]\n",
      " [1111 1217]\n",
      " [1218 1497]\n",
      " [1498 1564]\n",
      " [1565 1617]\n",
      " [1618 1682]\n",
      " [1683 1725]\n",
      " [1726 2170]\n",
      " [2171 2212]\n",
      " [2213 2279]\n",
      " [2280 2338]\n",
      " [2339 2424]\n",
      " [2425 2572]\n",
      " [2573 2669]\n",
      " [2670 2754]\n",
      " [2755 2826]\n",
      " [2827 3236]\n",
      " [3237 3341]\n",
      " [3342 4011]\n",
      " [4012 4083]\n",
      " [4084 4144]\n",
      " [4145 4204]\n",
      " [4205 4299]\n",
      " [4300 4387]\n",
      " [4388 4413]\n",
      " [4414 4471]\n",
      " [4472 4579]\n",
      " [4580 4639]\n",
      " [4640 4832]\n",
      " [4833 4896]\n",
      " [4897 5115]\n",
      " [5116 5252]\n",
      " [5253 5444]\n",
      " [5445 5557]\n",
      " [5558 5658]\n",
      " [5659 5677]\n",
      " [5678 5770]\n",
      " [5771 5871]\n",
      " [5872 6012]\n",
      " [6013 6070]\n",
      " [6071 6256]\n",
      " [6257 6448]\n",
      " [6449 6479]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # shape: batch dim x video frames x frame height x frame width x RGB (not BGR) channels\n",
    "    scenes = predict_video('../../Dataset/00100.mp4', model)\n",
    "    print(\"Detected scenes:\", scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10881188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.11.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc9f82cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TransNetV2] Processing video frames 6824/6824"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image, ImageTk\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "\n",
    "def extract_frame(video_path, frame_idx):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        return None\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(frame_rgb)\n",
    "\n",
    "def visualize_shots_scrollable(video_path, scenes_data):\n",
    "    if isinstance(scenes_data, torch.Tensor):\n",
    "        scenes_data = scenes_data.cpu().numpy()\n",
    "    elif isinstance(scenes_data, list):\n",
    "        scenes_data = np.array(scenes_data)\n",
    "\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Shot Boundary Viewer\")\n",
    "    def on_closing():\n",
    "        root.quit()\n",
    "        root.destroy()\n",
    "\n",
    "    root.protocol(\"WM_DELETE_WINDOW\", on_closing)\n",
    "\n",
    "    main_frame = ttk.Frame(root)\n",
    "    main_frame.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "    canvas = tk.Canvas(main_frame)\n",
    "    scrollbar = ttk.Scrollbar(main_frame, orient=tk.VERTICAL, command=canvas.yview)\n",
    "    scrollable_frame = ttk.Frame(canvas)\n",
    "\n",
    "    scrollable_frame.bind(\n",
    "        \"<Configure>\",\n",
    "        lambda e: canvas.configure(\n",
    "            scrollregion=canvas.bbox(\"all\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    canvas.create_window((0, 0), window=scrollable_frame, anchor=\"nw\")\n",
    "    canvas.configure(yscrollcommand=scrollbar.set)\n",
    "\n",
    "    canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "    scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "\n",
    "    for idx, (start, end) in enumerate(scenes_data):\n",
    "        start_img = extract_frame(video_path, int(start))\n",
    "        end_img = extract_frame(video_path, int(end))\n",
    "\n",
    "        if start_img is None or end_img is None:\n",
    "            continue\n",
    "\n",
    "        start_img_tk = ImageTk.PhotoImage(start_img.resize((160, 90)))\n",
    "        end_img_tk = ImageTk.PhotoImage(end_img.resize((160, 90)))\n",
    "\n",
    "        pair_frame = ttk.Frame(scrollable_frame, padding=10)\n",
    "        pair_frame.pack(fill=tk.X)\n",
    "\n",
    "        ttk.Label(pair_frame, text=f\"Shot {idx+1}: [{start}-{end}]\").pack(anchor=\"w\")\n",
    "\n",
    "        images_frame = ttk.Frame(pair_frame)\n",
    "        images_frame.pack()\n",
    "\n",
    "        ttk.Label(images_frame, image=start_img_tk).pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Label(images_frame, image=end_img_tk).pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        # Store reference to avoid garbage collection\n",
    "        pair_frame.start_img_tk = start_img_tk\n",
    "        pair_frame.end_img_tk = end_img_tk\n",
    "\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    video_file = '../../Dataset/00001.mp4'\n",
    "    #example_scenes = [[0, 433], [434, 495], [496, 553], [554, 646]]\n",
    "    with torch.no_grad():\n",
    "    # shape: batch dim x video frames x frame height x frame width x RGB (not BGR) channels\n",
    "        scenes = predict_video(video_file, model)\n",
    "    scenes_np = np.array(scenes)\n",
    "\n",
    "    if os.path.exists(video_file):\n",
    "        visualize_shots_scrollable(video_file, scenes_np)\n",
    "    else:\n",
    "        print(f\"Video file not found: {video_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VCA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
